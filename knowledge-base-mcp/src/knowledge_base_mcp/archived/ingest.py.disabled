import asyncio
import logging
from datetime import UTC, datetime
from io import BytesIO
from typing import Annotated

from crawlee import ConcurrencySettings, Glob
from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext
from crawlee.statistics import FinalStatistics
from docling_core.types.io import DocumentStream
from fastmcp import Context
from fastmcp.contrib.mcp_mixin.mcp_mixin import MCPMixin, mcp_tool
from llama_index.core import Document
from llama_index.core.indices.vector_store import VectorStoreIndex
from llama_index.core.ingestion.pipeline import IngestionPipeline
from llama_index.core.schema import BaseNode
from llama_index.readers.docling import DoclingReader
from pydantic import BaseModel, Field

from knowledge_base_mcp.debug.util import BASE_LOGGER
from knowledge_base_mcp.ingest_pipelines.docling import get_docling_prep_pipeline

logger = BASE_LOGGER.getChild(__name__)


KnowledgeBase = Annotated[
    str, Field(description="The name of the Knowledge Base to create to store this webpage.", examples=["Python 3.12 Documentation"])
]
URLs = Annotated[
    list[str],
    Field(
        description="The URLs to crawl and add to the knowledge base.",
        examples=["https://www.python.org/docs/3.12/"],
        min_length=1,
        max_length=100,
    ),
]

Recurse = Annotated[
    bool,
    Field(
        description="Whether to crawl the URLs recursively. Only URLs which start with the provided URLs will be crawled (i.e. child pages).",
    ),
]

Background = Annotated[
    bool,
    Field(
        description="Whether to run the crawl in the background. If True, the crawl will be queued and the response will be a message indicating that the task has been queued.",
    ),
]


class CrawlResult(BaseModel):
    url: str = Field(description="The URL of the crawled page.")
    title: str = Field(description="The title of the crawled page.")


class CrawlResponse(BaseModel):
    success: bool = Field(description="Whether the crawl was successful.")
    pages: int = Field(description="The number of pages crawled.")
    # chunks: int = Field(description="The number of chunks created.")
    duration_ms: int = Field(description="The duration of the crawl in milliseconds.")
    statistics: FinalStatistics = Field(description="The statistics of the crawl.")


class CrawlTaskStatus(BaseModel):
    completed: list[CrawlResponse]
    in_progress: int


class CrawlerTask:
    task: asyncio.Task[CrawlResponse]
    total: int
    completed: int

    @property
    def progress(self) -> float:
        return self.completed / self.total


class HtmlIngestTimer(BaseModel):
    start_docling: datetime
    start_cleanup: datetime
    start_embeddings: datetime
    start_vector_store: datetime
    end_time: datetime

    @property
    def docling_time(self) -> float:
        return (self.start_cleanup - self.start_docling).total_seconds()

    @property
    def cleanup_time(self) -> float:
        return (self.start_embeddings - self.start_cleanup).total_seconds()

    @property
    def embeddings_time(self) -> float:
        return (self.start_vector_store - self.start_embeddings).total_seconds()

    @property
    def vector_store_time(self) -> float:
        return (self.end_time - self.start_vector_store).total_seconds()

    @property
    def total_time(self) -> float:
        return (self.end_time - self.start_docling).total_seconds()


class HtmlTimerSummary(BaseModel):
    total_time: float
    docling_time: float
    cleanup_time: float
    embeddings_time: float
    vector_store_time: float

    @classmethod
    def from_timers(cls, timers: list[HtmlIngestTimer]) -> "HtmlTimerSummary":
        return cls(
            total_time=sum(timer.total_time for timer in timers),
            docling_time=sum(timer.docling_time for timer in timers),
            cleanup_time=sum(timer.cleanup_time for timer in timers),
            embeddings_time=sum(timer.embeddings_time for timer in timers),
            vector_store_time=sum(timer.vector_store_time for timer in timers),
        )


class IngestServer(MCPMixin):
    current_tasks: list[asyncio.Task[CrawlResponse]]

    def __init__(self, vector_store_index: VectorStoreIndex, tokenizer):
        super().__init__()
        self.vector_store_index = vector_store_index
        self.vector_store = vector_store_index.vector_store
        self.embeddings_model = vector_store_index._embed_model

        self.embeddings_pipeline: IngestionPipeline = IngestionPipeline(
            transformations=[
                embedding_model,
            ]
        )

        # Configuration.persist_storage = False

        self.docling_prep_pipeline: IngestionPipeline = get_docling_prep_pipeline(tokenizer=tokenizer)

        self.reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)
        docling_logger = logging.getLogger("docling")
        docling_logger.setLevel(logging.WARNING)
        # beautifulsoup_logger = logging.getLogger("BeautifulSoupCrawler")

        self.current_tasks = []

    async def _crawl_webpage(self, fastmcp_context: Context, knowledge_base: str, urls: list[str], recurse: bool = True) -> CrawlResponse:
        concurrency_settings = ConcurrencySettings(
            min_concurrency=4,
            desired_concurrency=8,
        )

        total_documents: int = 0
        total_nodes: int = 0
        timers: list[HtmlIngestTimer] = []

        reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)

        async def _report_request_start(url: str) -> None:
            request_manager = await crawler.get_request_manager()
            handled = await request_manager.get_handled_count()
            total = await request_manager.get_total_count()
            msg = f"Processing {url} ... ({handled}/{total})"
            logger.info(msg)
            await fastmcp_context.report_progress(progress=handled, total=total, message=msg)

        async def _report_request_end(url: str, nodes: list[BaseNode], timer: HtmlIngestTimer) -> None:
            msg = f"Done processing {len(nodes)} nodes for {url} in {timer}"
            nonlocal total_documents
            nonlocal total_nodes
            nonlocal timers

            total_documents += len(nodes)
            total_nodes += len(nodes)
            timers.append(timer)
            logger.info(msg)

        async def handle_crawled_page(context: BeautifulSoupCrawlingContext) -> None:
            await _report_request_start(context.request.url)
            title = str(context.soup.title.string) if context.soup.title else None
            source = context.request.url
            content = context.http_response._response.content  # type: ignore

            nodes, timer = await self._ingest_html(html=content, title=title, source=source, knowledge_base=knowledge_base)

            await _report_request_end(context.request.url, nodes, timer)

            # Enqueue all links found on the page that point to a child page
            if recurse:
                await context.enqueue_links(
                    include=[Glob(f"{context.request.url}/**"), Glob(f"{context.request.url}/*"), Glob(f"{context.request.url}*")]
                )

        crawler = BeautifulSoupCrawler(
            max_requests_per_crawl=1000,
            concurrency_settings=concurrency_settings,
            request_handler=handle_crawled_page,
            configure_logging=False,
        )

        start_time = datetime.now(tz=UTC)

        statistics = await crawler.run(urls)

        duration_ms: int = int((datetime.now(tz=UTC) - start_time).total_seconds() * 1000)

        response = CrawlResponse(success=True, pages=statistics.requests_total, duration_ms=duration_ms, statistics=statistics)

        logger.info(f"Finished crawling {statistics.requests_total} pages in {duration_ms}ms: {response}")

        logger.info(f"Total time taken: {HtmlTimerSummary.from_timers(timers)}")

        return response

    async def _ingest_html(self, html: str, title: str | None, source: str, knowledge_base: str) -> tuple[list[BaseNode], HtmlIngestTimer]:
        """Ingest HTML into the knowledge base."""

        docling_start_time = datetime.now(tz=UTC)

        doc_stream = DocumentStream(name="webpage", stream=BytesIO(html))  # type: ignore

        root_pages: list[Document] = await self.reader.aload_data(file_path=[doc_stream])

        for root_page in root_pages:
            root_page.metadata["title"] = title
            root_page.metadata["source"] = source
            root_page.metadata["fetched_at"] = datetime.now(tz=UTC).isoformat()
            root_page.metadata["source_type"] = "webpage"
            root_page.metadata["knowledge_base"] = knowledge_base

        document_cleanup_start_time = datetime.now(tz=UTC)

        nodes: list[BaseNode] = list(await self.docling_prep_pipeline.arun(documents=root_pages))

        embeddings_start_time = datetime.now(tz=UTC)

        await self.embeddings_pipeline.arun(nodes=nodes)

        vector_store_start_time = datetime.now(tz=UTC)

        await self.vector_store_index.vector_store.async_add(nodes=list(nodes))

        end_time = datetime.now(tz=UTC)

        return nodes, HtmlIngestTimer(
            start_docling=docling_start_time,
            start_cleanup=document_cleanup_start_time,
            start_embeddings=embeddings_start_time,
            start_vector_store=vector_store_start_time,
            end_time=end_time,
        )

    @mcp_tool()
    async def crawl_webpage(
        self, fastmcp_context: Context, knowledge_base: KnowledgeBase, urls: URLs, recurse: Recurse, background: Background
    ) -> str | CrawlResponse:
        """Crawl a webpage and add it to the knowledge base."""

        if background:
            task = asyncio.create_task(self._crawl_webpage(fastmcp_context, knowledge_base, urls, recurse))
            self.current_tasks.append(task)
            return "Task has been queued."

        return await self._crawl_webpage(fastmcp_context, knowledge_base, urls, recurse)

    @mcp_tool()
    async def check_tasks(self) -> CrawlTaskStatus:
        """Gather any completed tasks and report the number of in progress tasks."""

        completed_tasks = []
        in_progress_tasks = []

        for task in self.current_tasks:
            if task.done():
                completed_tasks.append(task.result())
            else:
                in_progress_tasks.append(task)

        return CrawlTaskStatus(completed=completed_tasks, in_progress=len(in_progress_tasks))
